# DataPurity Core - Contact Cleaning Engine

A production-grade Python module for cleaning, normalizing, scoring, and deduplicating contact data with full Arabic and English support.

## Features

âœ¨ **Comprehensive Cleaning**

- Text normalization (whitespace, control characters, zero-width chars)
- Name normalization with Arabic support
- Phone number normalization using `phonenumbers` library
- Email validation with bad domain filtering

ðŸ“ž **Smart Phone Handling**

- E.164 format normalization
- Saudi Arabia mobile format support (05XXXXXXXX â†’ +966XXXXXXXXX)
- International phone number validation
- Configurable country codes

âœ‰ï¸ **Email Validation**

- RFC-compliant email regex
- Bad domain filtering
- Case normalization

ðŸ” **Advanced Deduplication**

- Hard deduplication (exact phone/email matches)
- Fuzzy name matching using RapidFuzz
- Configurable similarity thresholds
- Duplicate group tracking

ðŸ“Š **Quality Scoring**

- Weighted scoring system (0-100)
- Field completeness tracking
- Data quality metrics

ðŸŒ **Arabic + English Support**

- Column name mapping (Arabic â†” English)
- Arabic text preservation
- UTF-8-SIG encoding support

## Installation

### Requirements

- Python 3.11+
- pandas
- phonenumbers
- rapidfuzz
- pydantic
- pydantic-settings
- openpyxl (for Excel files)

### Install Dependencies

```bash
pip install pandas phonenumbers rapidfuzz pydantic pydantic-settings openpyxl
```

For API:

```bash
pip install fastapi uvicorn python-multipart
```

For testing:

```bash
pip install pytest
```

## Quick Start

### 1. Command-Line Usage

```bash
# Basic cleaning
python -m scripts.datapurity_clean_cli input.xlsx output.xlsx

# With custom country code
python -m scripts.datapurity_clean_cli contacts.csv cleaned.csv --country SA

# Disable fuzzy deduplication
python -m scripts.datapurity_clean_cli data.xlsx clean.xlsx --no-fuzzy

# Custom fuzzy threshold (0-100)
python -m scripts.datapurity_clean_cli data.xlsx clean.xlsx --fuzzy-threshold 95

# Verbose logging
python -m scripts.datapurity_clean_cli input.xlsx output.xlsx --verbose
```

### 2. Python API

```python
import pandas as pd
from datapurity_core.config import get_settings
from datapurity_core.cleaning import clean_contacts_df
from datapurity_core.io_utils import load_contacts_file, save_contacts_file

# Load settings
settings = get_settings()

# Customize settings
settings.DEFAULT_COUNTRY_CODE = "SA"
settings.FUZZY_NAME_THRESHOLD = 90

# Load data
df = load_contacts_file("contacts.xlsx")

# Clean data
df_cleaned, stats = clean_contacts_df(df, settings)

# Save results
save_contacts_file(df_cleaned, "cleaned_contacts.xlsx")

# Print statistics
print(f"Original: {stats.rows_original}")
print(f"Final: {stats.rows_final}")
print(f"Duplicates removed: {stats.duplicates_removed}")
print(f"Avg quality score: {stats.avg_quality_score:.1f}")
```

### 3. FastAPI Web Service

```bash
# Start server
cd backend/api
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Upload and clean file:**

```bash
curl -X POST "http://localhost:8000/clean" \
  -F "file=@contacts.xlsx" \
  -F "country_code=SA" \
  -F "enable_fuzzy=true" \
  -F "fuzzy_threshold=90"
```

**Response:**

```json
{
  "task_id": "123e4567-e89b-12d3-a456-426614174000",
  "original_filename": "contacts.xlsx",
  "stats": {
    "rows_original": 1000,
    "rows_final": 850,
    "duplicates_removed": 120,
    "empty_rows_removed": 30,
    "phone_valid_count": 800,
    "phone_invalid_count": 50,
    "email_valid_count": 750,
    "email_invalid_count": 100,
    "avg_quality_score": 85.3
  },
  "download_url": "/clean/download/123e4567-e89b-12d3-a456-426614174000"
}
```

**Download cleaned file:**

```bash
curl -O "http://localhost:8000/clean/download/{task_id}"
```

## Configuration

### Environment Variables

Create `.env` file:

```env
DEFAULT_COUNTRY_CODE=SA
FUZZY_NAME_THRESHOLD=90
FUZZY_EMAIL_THRESHOLD=85
MIN_VALID_NAME_LEN=3
ENABLE_FUZZY_DEDUP=true
BAD_EMAIL_DOMAINS=test.com,example.com,spam.com
```

### Settings Class

```python
from datapurity_core.config import Settings, get_settings

# Get singleton settings
settings = get_settings()

# Override settings
settings.DEFAULT_COUNTRY_CODE = "AE"
settings.FUZZY_NAME_THRESHOLD = 95
```

## Input File Format

### Required Columns

The system accepts both Arabic and English column names:

| English     | Arabic                           | Description   |
| ----------- | -------------------------------- | ------------- |
| `name`      | `Ø§Ù„Ø§Ø³Ù…`, `Ø§Ù„Ø£Ø³Ù…`, `Ø§Ø³Ù…`          | Contact name  |
| `phone`     | `Ø§Ù„Ø¬ÙˆØ§Ù„`, `Ø§Ù„Ù‡Ø§ØªÙ`, `Ø±Ù‚Ù… Ø§Ù„Ø¬ÙˆØ§Ù„` | Phone number  |
| `email`     | `Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ`, `Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„`   | Email address |
| `company`   | `Ø§Ù„Ø´Ø±ÙƒØ©`, `Ø§Ø³Ù… Ø§Ù„Ø´Ø±ÙƒØ©`           | Company name  |
| `job_title` | `Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ`, `Ø§Ù„ÙˆØ¸ÙŠÙØ©`      | Job title     |
| `city`      | `Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©`                        | City          |
| `notes`     | `Ù…Ù„Ø§Ø­Ø¸Ø§Øª`                        | Notes         |

### Supported File Types

- Excel (`.xlsx`, `.xls`)
- CSV (`.csv`)

### Example Input

**Excel/CSV:**

```
Ø§Ù„Ø§Ø³Ù…,Ø§Ù„Ø¬ÙˆØ§Ù„,Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ,Ø§Ù„Ø´Ø±ÙƒØ©
Ø£Ø­Ù…Ø¯ Ù…Ø­Ù…Ø¯,0501234567,ahmed@example.com,Ø´Ø±ÙƒØ© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª
Ahmed Ali,+966509876543,ali@test.com,Acme Corp
```

## Output Format

### Cleaned Columns

- `id` - Unique row ID
- `name` - Normalized name
- `phone` - E.164 formatted phone (+966XXXXXXXXX)
- `email` - Lowercase normalized email
- `company` - Cleaned company name
- `job_title` - Cleaned job title
- `city` - Cleaned city name
- `notes` - Preserved notes
- `phone_valid` - Boolean flag
- `email_valid` - Boolean flag
- `quality_score` - Quality score (0-100)
- `is_duplicate` - Duplicate flag
- `duplicate_group_id` - Group ID for related duplicates
- `duplicate_reason` - Reason for duplication

## Quality Scoring

**Breakdown (Total: 100 points)**

| Field                 | Points |
| --------------------- | ------ |
| Valid phone           | 30     |
| Valid email           | 30     |
| Valid name (â‰¥3 chars) | 20     |
| Company provided      | 10     |
| Job title provided    | 5      |
| City provided         | 5      |

**Example:**

- Contact with phone, email, name, company = 90 points
- Contact with only phone and name = 50 points

## Deduplication Logic

### Hard Duplicates

**Criteria:**

1. Exact phone number match
2. Exact email address match

**Strategy:**

- First occurrence kept
- Subsequent duplicates marked and removed

### Fuzzy Duplicates (Optional)

**Method:** RapidFuzz Levenshtein ratio

**Threshold:** Configurable (default: 90%)

**Example:**

- "Ahmed Mohamed" vs "Ahmed Mohammed" â†’ 95% similarity â†’ Duplicate

## Statistics Output

```python
CleaningStats(
    rows_original=1000,
    rows_final=850,
    duplicates_removed=120,
    empty_rows_removed=30,
    phone_valid_count=800,
    phone_invalid_count=50,
    email_valid_count=750,
    email_invalid_count=100,
    avg_quality_score=85.3
)
```

## Testing

### Run All Tests

```bash
pytest tests/
```

### Run Specific Test File

```bash
pytest tests/test_cleaning.py -v
pytest tests/test_deduplication.py -v
pytest tests/test_scoring.py -v
```

### Test Coverage

```bash
pytest --cov=datapurity_core tests/
```

## Module Structure

```
datapurity_core/
â”œâ”€â”€ __init__.py          # Package initialization
â”œâ”€â”€ config.py            # Settings with Pydantic
â”œâ”€â”€ models.py            # Data models
â”œâ”€â”€ io_utils.py          # File I/O and column normalization
â”œâ”€â”€ cleaning.py          # Core cleaning logic
â”œâ”€â”€ deduplication.py     # Duplicate detection
â”œâ”€â”€ scoring.py           # Quality scoring
â””â”€â”€ stats.py             # Statistics calculation

scripts/
â””â”€â”€ datapurity_clean_cli.py  # CLI tool

api/
â””â”€â”€ main.py              # FastAPI application

tests/
â”œâ”€â”€ test_cleaning.py
â”œâ”€â”€ test_deduplication.py
â””â”€â”€ test_scoring.py
```

## Common Use Cases

### 1. Clean Saudi Contacts

```python
from datapurity_core.config import get_settings
from datapurity_core.cleaning import clean_contacts_df
from datapurity_core.io_utils import load_contacts_file, save_contacts_file

settings = get_settings()
settings.DEFAULT_COUNTRY_CODE = "SA"

df = load_contacts_file("saudi_contacts.xlsx")
df_cleaned, stats = clean_contacts_df(df, settings)
save_contacts_file(df_cleaned, "cleaned.xlsx")
```

### 2. High-Quality Contacts Only

```python
df_cleaned, stats = clean_contacts_df(df, settings)

# Filter by quality score
high_quality = df_cleaned[df_cleaned["quality_score"] >= 80]

save_contacts_file(high_quality, "high_quality_contacts.xlsx")
```

### 3. Export Valid Phones Only

```python
df_cleaned, stats = clean_contacts_df(df, settings)

# Filter valid phones
valid_phones = df_cleaned[df_cleaned["phone_valid"] == True]

# Export to CSV
save_contacts_file(valid_phones[["name", "phone"]], "valid_phones.csv")
```

### 4. Aggressive Deduplication

```python
settings = get_settings()
settings.ENABLE_FUZZY_DEDUP = True
settings.FUZZY_NAME_THRESHOLD = 85  # More aggressive

df_cleaned, stats = clean_contacts_df(df, settings)
```

## Best Practices

1. **Always backup original data** before cleaning
2. **Review statistics** after cleaning to verify results
3. **Adjust fuzzy threshold** based on your data quality
4. **Use quality scores** to filter contacts
5. **Test with sample data** before processing large files
6. **Handle Arabic text** with UTF-8-SIG encoding

## Troubleshooting

### Issue: Arabic text appears garbled

**Solution:** Use UTF-8-SIG encoding when saving CSV files:

```python
df.to_csv("output.csv", encoding="utf-8-sig", index=False)
```

### Issue: Phone numbers not normalizing

**Solution:** Check country code setting:

```python
settings.DEFAULT_COUNTRY_CODE = "SA"  # For Saudi Arabia
```

### Issue: Too many duplicates being marked

**Solution:** Increase fuzzy threshold:

```python
settings.FUZZY_NAME_THRESHOLD = 95  # More strict
```

### Issue: Valid contacts being removed

**Solution:** Lower minimum name length:

```python
settings.MIN_VALID_NAME_LEN = 2
```

## Performance

- **Processing speed:** ~10,000 rows/second (basic cleaning)
- **Fuzzy dedup:** ~1,000 rows/second (O(nÂ²) complexity)
- **Memory usage:** ~100MB per 10,000 rows

**Recommendations:**

- Disable fuzzy dedup for files >50,000 rows
- Process large files in batches
- Use CLI for batch processing

## License

MIT License

## Support

For issues, questions, or contributions, please contact the development team.

---

**DataPurity Core** - Professional Contact Data Cleaning for Arabic & English Markets
